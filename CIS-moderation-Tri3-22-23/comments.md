---
PART 1
---

# General questions

The second examiner box on front page of exam scripts (pen & paper) is
not used?  Are these scripts moderated/second marked (cf MOD002641 and
MOD006561)?

> Answer: Our regulations do not require second marking for exams, but ‘internal moderation’, which means that the internal moderator takes a close look at how the marker has applied the marking scheme (e.g. fairness, consistency across the sample) and also checks the administrative aspects of marking (e.g. correctly adding up the marks, not missing any question, etc.). If all these aspects are satisfactory, the internal moderator agrees the marking. If issues are discovered, the first marker is required to address them – if discrepancies are large, the moderator can ask for all the papers to be remarked or to recommend a third colleague to be involved in the moderation process.
> Before the pandemic, the custom was that the internal moderator will use a green pen to either show agreement with marks or issues on the sample exam scripts. Now that the MAP samples are in pdf format, it hasn’t always been possible for the internal moderator to put a physical pen on the exam scripts; rather we use the internal moderation form Part A to record the agreement of marking (or the various stages/iterations before agreement is reached, if that is the case).

# MOD002641 - Data Structures and Algorithms

The work has been marked fairly and consistently. 

## Main exam script (element 010):
ok but how are these scripts second marked (see question above)?

## Coursework (element 011):
ok.

# MOD002726 - Postgraduate Major Project

The work has been marked fairly and consistently. 

## Trimester 1

how does the internal moderation process work? I am surprised all 3
marks are the same for each student in the module report (this doesn't
seem to match marks from 1st/2nd marker in the feedback, nor the
report for Trimester 2). Is there an additional moderation step?

> All marks on the moderation form are the final marks. This is wrong. The marks for the supervisor and the 2nd marker should have been added. The moderation form has been updated accordingly.
  
## Trimester 2
ok.


# MOD003264 - Digital Security

The work has been marked fairly and consistently. 


Three locations: CAM – HCT – CWK. Very small numbers outside of CAM
(180 vs 5) so hard to compare.

# MOD004364 - Advanced Web Solutions

The work has been marked fairly and consistently. 


# MOD005618 - Object Orientated Modelling and Data Design

The work has been marked fairly and consistently. 


# MOD005622 - Object Orientated Software Development

The work has been marked fairly and consistently. 


# MOD006561 - Artificial Neural Networks

* Was low attendance also a problem in other modules for this cohort?

* Coursework: ok

* Exam: ok but how are these scripts second marked (see question above)?


# MOD007357 - Algorithm Analysis and Data Structures

The work has been marked fairly and consistently. 

* I am not sure whether feedback on module amendment are needed at
  this stage, but the proposal here makes sense to me. 
  

# Global comments:

- second marking could be more clearly shown

- add distribution on moderation reports

- quite high number of non-submissions

- it would be good to present MAP boxes accross modules and
  locations. For instance moderation samples for MOD003264 are given
  in three quite different styles for each location. Also for
  MOD005623, all scripts could be hyper linked from the moderation
  form (not just CAM ones).
  All locations should follow the Cambridge style, ideally. Links to
  samples (incl feedback) from moderation form is most convenient.

- MOD007357: lots of cheating and poor attendance, potentially due to
  "COVID cohort". Want to move to more invigilated assessments (also
  bc of ChatGPT etc).
  
- Borderline marks (39%) should be checked before? 


---
PART 2
---


# MOD002701 - Developing Web Applications


## Written assignment 4000

The work has been marked fairly and consistently. 

## Demonstration 15 mins

The link point the the assignment above (couldn't find related docs on canvas).


# MOD005623 - Cloud Based Application Development and Security
Ok.

## Coursework (3000 word or equivalent)

The work has been marked fairly and consistently. 

Note to self: 2 (out of 5) students submitted MES, some seems unhappy
about module delivery (tutor is blamed as module leader not involved
in delivery). Location: CAM.

# MOD006128 - Distributed Programming

Ok.

## Coding artefact and report (3000 word equivalent)

The work has been marked fairly and consistently. 

Note to self: location/numbers = CAM 29, CWA 2, HCT 19


# MOD003263 - Software Engineering

The work has been marked fairly and consistently. The marks seems
unusually low. Is this common for this module? Was this cohort
particularly weak? Did the student survey reveal anything?

NB: there are two .csv files in this folder, I am not sure what each
correspond to (both are about Element 010?).

---
MAP meeting notes
---


## Things to note 

* no boycott-related missing marks

* Want: 80% pass rate and ~60% average

* Apprentiship still does very well (not spread) but this can be
  explained by students just being committed (>< typical UG students).
  
* Attendance weak, some very weak students

* A few instances of asking for assessments to be remarked (eg from
  SAM - Trinidad)

## Questions

* Plans to monitor engagement? (address number of non-submission)



## Overall comments

* Excellent feedback, good spread generally

* Usually, nice MAP boxes (consistent)

* Attendance / low engagement in level 5 unfortunate -> think about
  how to suppor tem next year?
